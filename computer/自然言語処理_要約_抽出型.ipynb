{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwCjVL1Da9zASw5ugsgjx2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasachichito/knowledge/blob/master/computer/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86_%E8%A6%81%E7%B4%84_%E6%8A%BD%E5%87%BA%E5%9E%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0rmvm7t6PLb"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[ja]==4.25 deep-translator spacy==3.4.4 ginza==5.2.0 ja_ginza==5.2.0 sumy tinysegmenter\n",
        "!python -m spacy download ja_core_news_trf\n",
        "!python -m spacy download ja_core_news_lg\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def webpage_to_text(url, selector):\n",
        "  text = ''\n",
        "  with urlopen(url) as res:\n",
        "      html = res.read().decode('UTF-8', 'ignore')\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "      # article = soup.find('div', class_=\"articleBody\")\n",
        "      # text = article.get_text(strip=True)\n",
        "      article = soup.select(selector)\n",
        "      text = ''\n",
        "      for p in article:\n",
        "        text += p.get_text(strip=True)\n",
        "      return text\n",
        "\n",
        "def trim_last_halfway_sentence(text, period_char):\n",
        "  if text.endswith(period_char):\n",
        "    return text\n",
        "\n",
        "  last_period_index = text.rfind(period_char)\n",
        "  return text[:last_period_index + 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 抽出型要約"
      ],
      "metadata": {
        "id": "3FX4WN-W6fzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
        "from sumy.summarizers.kl import KLSummarizer\n",
        "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
        "from sumy.utils import get_stop_words\n",
        "import spacy\n",
        "\n",
        "LANGUAGE=\"japanese\"\n",
        "\n",
        "text = webpage_to_text('https://xtech.nikkei.com/atcl/nxt/column/18/02252/051400006/', '.articleBody p')\n",
        "text = trim_last_halfway_sentence(text[:800], '。')\n",
        "\n",
        "# Spacy ja_core_news_trf\n",
        "class SpacyJcntTokenizer(SumyTokenizer):\n",
        "  nlp = spacy.load(\"ja_core_news_trf\")\n",
        "  def to_words(self, text):\n",
        "    doc = self.nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Spacy GiNZA\n",
        "class SpacyGinzaTokenizer(SumyTokenizer):\n",
        "  nlp = spacy.load(\"ja_ginza\")\n",
        "  def to_words(self, text):\n",
        "    doc = self.nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "parser_s = PlaintextParser.from_string(text, SumyTokenizer(LANGUAGE))\n",
        "parser_sj = PlaintextParser.from_string(text, SpacyJcntTokenizer(LANGUAGE))\n",
        "parser_sg = PlaintextParser.from_string(text, SpacyGinzaTokenizer(LANGUAGE))\n",
        "\n",
        "summarizers = {\n",
        "    \"LexRank\": LexRankSummarizer(),\n",
        "    \"LSA\": LsaSummarizer(),\n",
        "    \"Luhn\": LuhnSummarizer(),\n",
        "    \"SumBasic\": SumBasicSummarizer(),\n",
        "    \"KL-Sum\": KLSummarizer(),\n",
        "    \"Edmundson\": EdmundsonSummarizer()\n",
        "}\n",
        "\n",
        "stop_words = get_stop_words(LANGUAGE)\n",
        "for name in summarizers:\n",
        "    summarizer = summarizers[name]\n",
        "    summarizer.stop_words = stop_words\n",
        "\n",
        "summarizers[\"Edmundson\"].bonus_words = frozenset([\"\"])\n",
        "summarizers[\"Edmundson\"].stigma_words = frozenset([\"\"])\n",
        "summarizers[\"Edmundson\"].null_words = frozenset([\"\"])\n",
        "\n",
        "summary_sentences = 3\n",
        "\n",
        "for name, summarizer in summarizers.items():\n",
        "    print(f\"{name} algorithm:\")\n",
        "    print(\"\\n\" + \">>>>> sumy tokenizer (default):\" + \"\\n\")\n",
        "    summary = summarizer(parser_s.document, summary_sentences)\n",
        "    for sentence in summary:\n",
        "        print(sentence)\n",
        "    print(\"\\n\" + \">>>>> ja_core_news_trf tokenizer:\" + \"\\n\")\n",
        "    summary = summarizer(parser_sj.document, summary_sentences)\n",
        "    for sentence in summary:\n",
        "        print(sentence)\n",
        "    print(\"\\n\" + \">>>>> ginza tokenizer:\" + \"\\n\")\n",
        "    summary = summarizer(parser_sg.document, summary_sentences)\n",
        "    for sentence in summary:\n",
        "        print(sentence)\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "CnBoINGr6j6w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}