<img src="/picture/自然言語処理1.png" width="100%" />

- コーパス
  - 新聞やネットの記事などの文章や、話し言葉を書き起こした文章などを大量に収集し、検索や分析ができるようにしたデータベースのこと。  
注釈付きコーパスとは、単語の分割境界や品詞タグなどの情報を付加したコーパスを指す。

- 語彙
  - トークンの集合

### 言語モデル
言語モデルとは、入力として文章（トークン列）を受け取り、文章の出現確率（もっともらしさ、出現しやすさ）を返す関数である。  
良いモデルは、人間がもっともらしいと感じる文章に高い確率、つまり1に近い数値を返却する。  
ニューラル言語モデルとは、ニューラルネットワークにより実現される言語モデルのこと。  

```
そもそも確率(Probability)とは、事象の起こりやすさを0以上1以下の数で表したもの。  
サイコロを振って1が出る確率をxとすると、2,3,4,5,6が出る確率も同じため（同様に確からしい、という）  
x + x + x + x + x + x = 1 が成り立ち、x = 1/6 となることからこれは理解できる。

確率の基本は、起こりうる事象を数え上げることにある。

サイコロを2回振って1回目に1、2回目に2が出る確率は、1/6 * 1/6 と覚えているが、これは分母同士の積で起こりうる事象の全パターンを数え上げ、
分子同士の積で対象の事象パターンを数え上げている。
```

言語モデルからすれば、入力された文章Sの出現確率は、Sを構成する複数のトークンが同時に出現する確率といえるため、各トークンの出現率の積を計算することになる。

各トークンの出現率だが、文章には流れ・文脈・テーマがあることから、あるトークンの出現率は、以前に出現したトークンの影響を受けることを考慮する必要がある。これには「条件付き確率」を使う。

以上から、文章Sが3つのトークンw1、w2, w3で構成される場合、文章Sの出現確率p(S)は、次のようになる。

`p(S) = p(w1,w2,w3) = p(w1) * p(w2|w1) * p(w3|w1,w2)`


### 分類器と言語モデル
分類器とは、入力データが与えられたとき、そのカテゴリーを予測するモデルのこと。  
モデルは、入力データとしてxを受け取り、出力としてカテゴリー数と同じ次元を持つベクトルyを返却する。  
yの中で最大値をとる次元に対応するカテゴリが答えとなる。

分類器の仕組みを言語モデルで利用すれば、ある文脈において、次に出現するトークンを予測することができる。  
つまり語彙に含まれる全てのトークン数がNであれば、カテゴリー数をNとするクラス分類と解釈する。

この使い方を言語モデルと表現することも多い。

モデルのパラメータを決定するための評価関数（損失関数）にはクロスエントロピー誤差を用いることが多い。  
これは出力yに非線形関数のSoftMax関数を適用した後、正解となるカテゴリに対応する次元の値を確認し、良し悪しを評価する。

### 学習
モデルを学習させるには、モデルに何かしらのタスクを行わせて（入力に対して出力させて）、その出力の良し悪しを評価する必要がある。  
その評価ができるだけ高くなるように（そのタスクを上手にできるように）モデル内部のパラメーターが調整されていく。  

従って、そもそも正解がわからない（出力を評価できない）タスクを行うモデルは、学習させられない。  
例えば、単語を入力したら、その分散表現を出力するモデルを作っても、正解がわからないから学習させられない。  
でも、文脈を入力したら、次に出てきそうな単語を予測するモデルを作れば、正解がわかるので学習させられる。  

後者のモデルが、内部で単語を分散表現で扱うようなアーキテクチャになっていれば、学習済みモデルに対し、その処理の部分だけ実行させることで、単語の分散表現を取得できる。  

いずれにせよ、モデルを考える上では、用いられる基礎的な理論、学習のために行うタスク（入力と出力）、モデルのアーキテクチャ、学習方法、学習済みモデルの使い方を分けて考えることが重要である。  

# ニューラル言語モデル

## Word2Vec
Mikolovらによって2013年に提案された、単語に対して文脈非依存の（一意な）分散表現を学習するモデル。  
単語に文脈非依存の分散表現を与えることを、文脈依存の分散表現と対比して、単語埋め込み（Word Embedding）と呼ぶ。　　

Word2Vecにより得られる分散表現は、単語間の意味的類似性だけでなく、`v(日本) - v(東京)　≒ v(フランス) - v(パリ)`、`v(日本) + v(首都) ≒ v(東京)`のような性質を示す。このような数値の加減算と意味的な加減算が一致する性質を加法構成性と呼ぶ。

Word2Vecでは、CBOW（シーバウ）、skip-gramの2つのモデルが提案されている。

ここでは主にSkip-Gramについて説明する。

### 分類器としてのモデルアーキテクチャと学習方法
- 入力：語彙に含まれるトークンを一つ指定するone-hotベクトル（一つの要素が1でそれ以外全てが0）
- 出力：語彙の各トークンの出現率を示す確率分布
- 論理処理フロー
  - ①語彙に含まれるトークン数をN、トークンの分散表現ベクトルの次元数をDとしたとき、N*Dの行列E（全トークンのベクトルを持つリスト）から、入力で指定されたトークンのベクトルを取得
  - ②語彙の各トークンについて、①のベクトルとの内積を算出してスコアリストを作成
  - ③スコアリストにSoftmax関数を適用して、語彙の各トークンの出現率を示す確率分布にして出力する
- パーセプトロン構成
  - <img src="/picture/自然言語処理2.png" width="50%" />
  - ①の行列Eから指定トークンのベクトルを取得する部分は、入力層でone-hotベクトルを受け取り、入力層の重み（各赤線に付与された重み）を対象トークンのベクトルにすることで実現している。
  - <img src="/picture/自然言語処理3.png" width="50%" />
  - ②の内積計算は、語彙の各トークンのベクトルを、中間層の重み（各青線に付与された重み）で表現することで実現している。
- モデルの学習方法
  - 入力：トークン一つ
  - 正解：入力のトークンの周りに出てくるトークン一つ
  - ハイパーパラメータ：周りの距離（トークン数）を定義するウィンドウサイズ
  - 正解のトークンに対して、高いスコアが出るように各層を調整する。

※CBOWはSkip-Gramの因果関係を逆転させたモデルで、入力として周辺のトークンリスト（multi-hotベクトル）を受け取り、出力として中心語の予測を返却する。  

## ELMo
2018年に提案されたモデル。Word2Vecとは異なり、ELMoから得られる単語の分散表現は、文脈に応じて異なる値をとる（文脈化単語埋め込み）。

順方向の多層LSTMと、逆方向の多層LSTMを組み合わせた、双方向LSTMで実現される。

### 分類器としてのモデルアーキテクチャと学習方法
- 入力：文章（トークンの列）
- 出力：語彙の各トークンの出現率を示す確率分布
- 論理処理フロー
  - a.入力文章の先頭トークンから順に以下を行う
    - ①トークンを文字単位に分割して、文字に割り振った値からベクトルを取得
    - ②次のトークンについて、文字単位に分割してベクトルを取得
    - ③一つ前のトークンのベクトルを②に加味したものをこのトークンのベクトルとする（ここが、文脈に応じてベクトルが変わるということ） 
    - ④②に戻る。入力文章の最後のトークンについて処理が終わったら次に進む。
    - ⑤語彙の各トークンについて、入力文章の最後のトークンのベクトルとの関係（内積等）を算出してスコアリストを作成（Softmax関数を適用した確率分布）
  - b.入力文章の末尾トークンから順に①〜⑤を行う。
  - c.a-⑤と、b-⑤を組み合わせた確率分布を出力する。
- パーセプトロン構成
  - Word2VecのSkip-Gramがたくさん並んで隣と連携しているイメージ（以下図では4つ）
  - <img src="/picture/自然言語処理5.png" width="50%" />
- 学習方法
  - 入力：文章（w1,w2,w3,w4,...,wn）
  - 正解：i番目のトークンwi
  - 正解のトークンに対して、高いスコアが出るように各層を調整する。

学習させたモデルを使えば、入力の各トークンに対する、文脈を考慮した単語分散表現を取得することができる。

<img src="/picture/自然言語処理4.png" width="50%" />

## BERT
BERTは2018年にGoogleにより提案されたモデルで、あるトークンを処理する際に、他のトークンの情報を直接参照することでLSTMよりも長期記憶を実現し、かつ、それぞれのトークン処理を並列実行できることから高い計算効率を誇る。

<img src="/picture/自然言語処理6.png" width="50%" />

BERTは、あるトークンの処理において、他のトークンの情報にどの程度注意を払うかを決めるための、Attention（注意機構）と呼ばれる仕組みを持つ。

BERTは、Transformerというモデルで提案されたTransformer EncoderをAttentionに用いたニューラルネットワーク言語モデルである。

### Transformer Encorder
Transformer Encorderは主に以下で構成される。
- `Multi-Head Attention`（`Scaled Dot-Product Attention`で構成される）
- `Layer Normalization`
- `Feed-forward Network`

<img src="/picture/自然言語処理7.png" width="50%" />


#### Scaled Dot-Product Attention
4つのトークンからなる文章（w1,w2,w3,w4）を入力したとき、w3から（あるいは一つ前の層での出力`x3`から）ベクトル`a3`を算出する際の`Scaled Dot-Product Attention`の流れは以下の通り。  

<img src="/picture/自然言語処理8.png" width="50%" />

- キー`k1`〜`k4`
  - w1〜w4に対応づけられた適当なベクトル
- クエリ`q3`
  - キーとの内積でw3との関係性を表すベクトル（`a3,1`がw3とw1との関係性を示す）
- バリュー`v1`〜`v4`
  - w1〜w4の値を調整するベクトル

#### Multi-Head Attention
`Scaled Dot-Product Attention`を複数回（異なるキー・クエリ・バリューの組を用いて）適用して、最後に出力を一つに集約する。

また、Transformer EncorderではResidual Connectionという接続形式を用いており、次の層にベクトル`a`をそのまま渡すのではなく、元の入力`x`を足してから渡す。これにより、深い層を持つモデルでも学習が適切に行われるようになる。

#### Layer Normalization
ベクトルを正規化して流す。

#### Feed-forward Network
GELU関数を適用して流す。

GELU関数は、ReLU関数をなめらかにしたような関数。



