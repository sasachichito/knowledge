{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzSYpwboLonoVeQ5mHdx3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasachichito/knowledge/blob/master/computer/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86_%E8%A6%81%E7%B4%84_%E6%8A%BD%E8%B1%A1%E5%9E%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 依存関係"
      ],
      "metadata": {
        "id": "Ksf_9_Ym5j5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv_Hda2H5MO0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers[ja] deep-translator\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def webpage_to_text(url, selector):\n",
        "  text = ''\n",
        "  with urlopen(url) as res:\n",
        "      html = res.read().decode('UTF-8', 'ignore')\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "      # article = soup.find('div', class_=\"articleBody\")\n",
        "      # text = article.get_text(strip=True)\n",
        "      article = soup.select(selector)\n",
        "      text = ''\n",
        "      for p in article:\n",
        "        text += p.get_text(strip=True)\n",
        "      return text\n",
        "\n",
        "def trim_last_halfway_sentence(text, period_char):\n",
        "  if text.endswith(period_char):\n",
        "    return text\n",
        "\n",
        "  last_period_index = text.rfind(period_char)\n",
        "  return text[:last_period_index + 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 抽象型要約ライブラリ比較"
      ],
      "metadata": {
        "id": "k9wYN8-75shn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "from transformers import pipeline\n",
        "\n",
        "def summary_text(text_ja, text_en):\n",
        "  print('==============origin================')\n",
        "  print(text_ja)\n",
        "  print(text_en)\n",
        "  print('====================================')\n",
        "\n",
        "  text = text_en\n",
        "\n",
        "  # GPU使える場合は使う\n",
        "  device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "  # 結果を日本語で確認するためにdeep_translatorを使用\n",
        "  translator_for_print_ja = GoogleTranslator(source='auto', target=\"ja\")\n",
        "\n",
        "  print(\"\\n\" + '>>>>> google/pegasus-large' + \"\\n\")\n",
        "  pega_l_summarizer = pipeline(\"summarization\", model=\"google/pegasus-large\", device=device)\n",
        "  summarized_pega_l = pega_l_summarizer(text, do_sample=False)\n",
        "  print([item['summary_text'] for item in summarized_pega_l][0])\n",
        "  print(translator_for_print_ja.translate([item['summary_text'] for item in summarized_pega_l][0]))\n",
        "\n",
        "  print(\"\\n\" + '>>>>> google/pegasus-xsum' + \"\\n\")\n",
        "  pega_x_summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\", device=device)\n",
        "  summarized_pega_x = pega_x_summarizer(text, do_sample=False)\n",
        "  print([item['summary_text'] for item in summarized_pega_x][0])\n",
        "  print(translator_for_print_ja.translate([item['summary_text'] for item in summarized_pega_x][0]))\n",
        "\n",
        "  print(\"\\n\" + '>>>>> facebook/bart-large-cnn' + \"\\n\")\n",
        "  bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
        "  summarized_bart = bart_summarizer(text, do_sample=False)\n",
        "  print([item['summary_text'] for item in summarized_bart][0])\n",
        "  print(translator_for_print_ja.translate([item['summary_text'] for item in summarized_bart][0]))\n",
        "\n",
        "  print(\"\\n\" + '>>>>> csebuetnlp/mT5_multilingual_XLSum' + \"\\n\")\n",
        "  t5_summarizer = pipeline(\"summarization\", model=\"csebuetnlp/mT5_multilingual_XLSum\", device=device)\n",
        "  summarized_t5 = t5_summarizer(text, do_sample=True)\n",
        "  print([item['summary_text'] for item in summarized_t5][0])\n",
        "  print(translator_for_print_ja.translate([item['summary_text'] for item in summarized_t5][0]))\n",
        "\n",
        "# 日本語記事の要約\n",
        "# text_ja = webpage_to_text('https://xtech.nikkei.com/atcl/nxt/column/18/02828/050900001/', '.articleBody p')\n",
        "# text_ja = webpage_to_text('https://xtech.nikkei.com/atcl/nxt/column/18/02783/051000018/', '.articleBody p')\n",
        "text_ja = webpage_to_text('https://xtech.nikkei.com/atcl/nxt/column/18/02252/051400006/', '.articleBody p')\n",
        "text_ja = trim_last_halfway_sentence(text_ja[:800], '。')\n",
        "translator_to_en = GoogleTranslator(source='auto', target=\"en\")\n",
        "text_en = translator_to_en.translate(text_ja)\n",
        "summary_text(text_ja, text_en)\n",
        "\n",
        "# 英語記事の要約\n",
        "text_en = webpage_to_text('https://edition.cnn.com/2024/05/13/politics/takeaways-michael-cohen-testimony-donald-trump-day-16/index.html', '.article__content p')\n",
        "text_en = trim_last_halfway_sentence(text_en[:800], '.')\n",
        "summary_text('', text_en)"
      ],
      "metadata": {
        "id": "fGRHjJVL5sMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 要約クラス実装"
      ],
      "metadata": {
        "id": "DwVF9qqAp4RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from deep_translator import GoogleTranslator\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "import torch\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "class ModelType(Enum):\n",
        "    facebook_bart_large_cnn = (\n",
        "        \"facebook/bart-large-cnn\",\n",
        "        512,\n",
        "        lambda tokens: np.append(np.insert(tokens, 0, 0), 2), # 先頭に0、末尾に2を入れる\n",
        "        1\n",
        "    )\n",
        "    google_pegasus_xsum = (\n",
        "        \"google/pegasus-xsum\",\n",
        "        512,\n",
        "        lambda tokens: np.append(tokens, 1),\n",
        "        0\n",
        "    )\n",
        "    csebuetnlp_mT5_multilingual_XLSum = (\n",
        "        \"csebuetnlp/mT5_multilingual_XLSum\",\n",
        "        512,\n",
        "        lambda tokens: np.append(tokens, 1),\n",
        "        0\n",
        "    )\n",
        "\n",
        "    def __init__(self, model_name, max_size, add_special_token_func, pad):\n",
        "        self.model_name = model_name\n",
        "        self.max_size = max_size\n",
        "        self.add_special_token_func = add_special_token_func\n",
        "        self.pad = pad\n",
        "\n",
        "    def add_special_token_func(self, tokens):\n",
        "      return self.add_special_token_func(tokens)\n",
        "\n",
        "\n",
        "class SummarizeSlot:\n",
        "    def __init__(self, model_type):\n",
        "      self.sentece_token_list = []\n",
        "      self.model_type = model_type\n",
        "\n",
        "    def can_append(self, append_size):\n",
        "      return self.size() + append_size <= self.model_type.max_size - 2\n",
        "\n",
        "    def get_token(self, index):\n",
        "      return self.sentece_token_list[index][1]\n",
        "\n",
        "    def pop(self):\n",
        "      return self.sentece_token_list.pop()\n",
        "\n",
        "    def append(self, sentence, tokens):\n",
        "      self.sentece_token_list.append([sentence, tokens])\n",
        "\n",
        "    def insert(self, index, sentence, tokens):\n",
        "      self.sentece_token_list.insert(index, [sentence, tokens])\n",
        "\n",
        "    def size(self):\n",
        "      return sum(row[1].size for row in self.sentece_token_list)\n",
        "\n",
        "    def tensor_2d(self):\n",
        "      token_array = np.concatenate([row[1] for row in self.sentece_token_list])\n",
        "      temp = self.model_type.add_special_token_func(token_array)\n",
        "      temp = np.pad( # max_sizeまでpadding\n",
        "          temp,\n",
        "          (0, self.model_type.max_size - temp.size),\n",
        "          constant_values=self.model_type.pad\n",
        "      )\n",
        "      return torch.from_numpy(temp.copy()).unsqueeze(0) # 2階テンソルで返却\n",
        "\n",
        "\n",
        "class SummarizeSlotList:\n",
        "    def __init__(self, text, delimiter, model_type):\n",
        "      self.text = text\n",
        "      self.delimiter = delimiter\n",
        "      self.model_type = model_type\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_type.model_name)\n",
        "      self.model = AutoModelForSeq2SeqLM.from_pretrained(model_type.model_name)\n",
        "      self.slot_size = model_type.max_size\n",
        "      self.slot_list = [SummarizeSlot(model_type)]\n",
        "\n",
        "      parts = text.split(delimiter)\n",
        "      sentence_list = [part + delimiter for part in parts]\n",
        "      [self.append(sentence) for sentence in sentence_list]\n",
        "\n",
        "    def append(self, sentence):\n",
        "      WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "      token_ids = self.tokenizer(\n",
        "          [WHITESPACE_HANDLER(sentence)],\n",
        "          return_tensors=\"np\",\n",
        "          add_special_tokens=False,\n",
        "          truncation=True,\n",
        "          max_length=512\n",
        "      )[\"input_ids\"][0]\n",
        "\n",
        "      slot = self.get_appendable_slot(token_ids.size)\n",
        "\n",
        "      slot.append(sentence, token_ids)\n",
        "\n",
        "    def get_appendable_slot(self, append_size):\n",
        "      last_slot = self.slot_list[-1]\n",
        "      if last_slot.can_append(append_size):\n",
        "        return last_slot\n",
        "      else:\n",
        "        new_slot = SummarizeSlot(self.model_type)\n",
        "        self.slot_list.append(new_slot)\n",
        "        return new_slot\n",
        "\n",
        "    def summarize_r(self, max_slot_cnt, **kwargs):\n",
        "      result = self.summarize(**kwargs)\n",
        "\n",
        "      for _ in range(3): # 最大{max_slot_cnt}回まで繰り返し要約する\n",
        "        if len(result) <= max_slot_cnt:\n",
        "            break\n",
        "        one_more = SummarizeSlotList(''.join(result), self.delimiter, self.model_type)\n",
        "        result = one_more.summarize(**kwargs)\n",
        "\n",
        "      return result\n",
        "\n",
        "    def summarize(self, **kwargs):\n",
        "      self.__flatten()\n",
        "\n",
        "      result = []\n",
        "      for slot in self.slot_list:\n",
        "        if torch.cuda.is_available():\n",
        "          self.model = self.model.cuda()\n",
        "          input_ids = slot.tensor_2d().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          output_ids = self.model.generate(\n",
        "              input_ids=input_ids,\n",
        "              **kwargs\n",
        "          )[0]\n",
        "\n",
        "        result.append(\n",
        "            self.tokenizer.decode(\n",
        "                output_ids,\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=False\n",
        "            )\n",
        "        )\n",
        "      return result\n",
        "\n",
        "    def slot_count(self):\n",
        "      return len(self.slot_list)\n",
        "\n",
        "    def __flatten(self):\n",
        "      if len(self.slot_list) <= 1:\n",
        "        return\n",
        "\n",
        "      last_slot = self.slot_list[-1]\n",
        "      last2_slot = self.slot_list[-2] # 最後から2番目のユニット\n",
        "\n",
        "      while last_slot.size() <= last2_slot.size():\n",
        "\n",
        "        if not last_slot.can_append(last2_slot.get_token(-1).size):\n",
        "          break\n",
        "\n",
        "        mv_sentece_token = last2_slot.pop()\n",
        "        last_slot.insert(0, mv_sentece_token[0], mv_sentece_token[1])\n",
        "\n",
        "def make_trans_batch_list(text, delimiter, size):\n",
        "  parts = text.split(delimiter)\n",
        "  sentence_list = [part + delimiter for part in parts]\n",
        "\n",
        "  trans_batch_list = []\n",
        "  batch = \"\"\n",
        "  for sentence in sentence_list:\n",
        "    if len(batch) + len(sentence) > size:\n",
        "      trans_batch_list.append(batch)\n",
        "      batch = \"\"\n",
        "\n",
        "    batch += sentence\n",
        "\n",
        "  if batch:\n",
        "    trans_batch_list.append(batch)\n",
        "\n",
        "  return trans_batch_list\n",
        "\n",
        "\n",
        "translator_to_en = GoogleTranslator(source='auto', target=\"en\")\n",
        "translator_to_ja = GoogleTranslator(source='auto', target=\"ja\")\n",
        "\n",
        "url_list = [\n",
        "    ['https://xtech.nikkei.com/atcl/nxt/column/18/02252/051400006/', '.articleBody p'],\n",
        "    ['https://xtech.nikkei.com/atcl/nxt/column/18/02252/052100016/', '.articleBody p'],\n",
        "    ['https://xtech.nikkei.com/atcl/nxt/column/18/00001/09325/', '.articleBody p'],\n",
        "    ['https://xtech.nikkei.com/atcl/nxt/column/18/00154/02062/', '.articleBody p'],\n",
        "    ['https://xtech.nikkei.com/atcl/nxt/column/18/00138/051601526/', '.articleBody p'],\n",
        "    ['https://ainow.ai/2024/05/28/276412/', '.article_area'],\n",
        "    ['https://aws.amazon.com/jp/blogs/news/gen-ai-usecase-daiichikosho/', 'article'],\n",
        "    ['https://techblog.lycorp.co.jp/ja/20240527a', 'article'],\n",
        "    ['https://prtimes.jp/main/html/rd/p/000000095.000034517.html', 'article'],\n",
        "    ['https://prtimes.jp/main/html/rd/p/000000094.000073671.html', 'article'],\n",
        "]\n",
        "\n",
        "text_ja_list = [webpage_to_text(url[0], url[1]) for url in url_list]\n",
        "\n",
        "text_en_list = []\n",
        "for text_ja in text_ja_list:\n",
        "  trans_batch_list = make_trans_batch_list(text_ja, '。', 2000)\n",
        "  text_en = ''.join(translator_to_en.translate(batch) for batch in trans_batch_list)\n",
        "  text_en_list.append(text_en)\n",
        "\n",
        "kwargs_list = [\n",
        "    # {\n",
        "    #     'name' : 'beam',\n",
        "    #     'kwargs' : {\n",
        "    #         'min_length': 10,\n",
        "    #         'max_length': 30,\n",
        "    #         # 'num_beams': 5,\n",
        "    #         'num_beams': 10,\n",
        "    #         # 'num_beams': 30,\n",
        "    #         'no_repeat_ngram_size': 2,\n",
        "    #         'early_stopping': True\n",
        "    #     }\n",
        "    # },\n",
        "    # {\n",
        "    #     'name' : 'sample_temp',\n",
        "    #     'kwargs' : {\n",
        "    #         'min_length': 10,\n",
        "    #         'max_length': 30,\n",
        "    #         'do_sample': True,\n",
        "    #         # 'temperature': 0.5,\n",
        "    #         'temperature': 0.7,\n",
        "    #         # 'temperature': 1.2,\n",
        "    #     }\n",
        "    # },\n",
        "    # {\n",
        "    #     'name' : 'sample_top_k',\n",
        "    #     'kwargs' : {\n",
        "    #         'min_length': 10,\n",
        "    #         'max_length': 30,\n",
        "    #         'do_sample': True,\n",
        "    #         # 'top_k': 30,\n",
        "    #         'top_k': 50,\n",
        "    #         # 'top_k': 100,\n",
        "    #     }\n",
        "    # },\n",
        "    # {\n",
        "    #     'name' : 'sample_top_p_0.7',\n",
        "    #     'kwargs' : {\n",
        "    #         'min_length': 10,\n",
        "    #         'max_length': 30,\n",
        "    #         'do_sample': True,\n",
        "    #         'top_p': 0.7,\n",
        "    #     }\n",
        "    # },\n",
        "    # {\n",
        "    #     'name' : 'sample_top_p_0.8',\n",
        "    #     'kwargs' : {\n",
        "    #         'min_length': 10,\n",
        "    #         'max_length': 30,\n",
        "    #         'do_sample': True,\n",
        "    #         'top_p': 0.8,\n",
        "    #     }\n",
        "    # },\n",
        "    # {\n",
        "    #     'name' : 'sample_top_p_0.9',\n",
        "    #     'kwargs' : {\n",
        "    #         'min_length': 10,\n",
        "    #         'max_length': 30,\n",
        "    #         'do_sample': True,\n",
        "    #         'top_p': 0.9,\n",
        "    #     }\n",
        "    # },\n",
        "    {\n",
        "        'name' : 'sample_top_p_0.95',\n",
        "        'kwargs' : {\n",
        "            'min_length': 10,\n",
        "            'max_length': 100,\n",
        "            'do_sample': True,\n",
        "            'top_p': 0.95,\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "for text_ja, text_en in zip(text_ja_list, text_en_list):\n",
        "  print(\"\\n\" + '-'*50 + \"\\n\" + text_ja + \"\\n\" + '-'*50)\n",
        "\n",
        "  summarize_slot_list = SummarizeSlotList(text_en, '.', ModelType.facebook_bart_large_cnn)\n",
        "  # summarize_slot_list = SummarizeSlotList(text_en, '.', ModelType.google_pegasus_xsum)\n",
        "  # summarize_slot_list = SummarizeSlotList(text_en, '.', ModelType.csebuetnlp_mT5_multilingual_XLSum)\n",
        "\n",
        "  for kwargs in kwargs_list:\n",
        "    print(\"\\n\" + kwargs.get('name') + \"\\n\")\n",
        "    result_list = summarize_slot_list.summarize_r(3, **kwargs.get('kwargs'))\n",
        "    # result_list = summarize_slot_list.summarize(**kwargs.get('kwargs'))\n",
        "    for result in result_list:\n",
        "      print(translator_to_ja.translate(result))\n"
      ],
      "metadata": {
        "id": "LsnCyIoPDNQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "スペシャルトークンの確認"
      ],
      "metadata": {
        "id": "rBUuRr2OFR_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# model_name = \"facebook/bart-large-cnn\"\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "# model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"スペシャルトークン:\")\n",
        "print(tokenizer.all_special_ids)\n",
        "print(tokenizer.all_special_tokens)\n",
        "\n",
        "# ID:105のトークン確認\n",
        "# print(tokenizer.decode(torch.tensor([105], dtype=torch.int32), skip_special_tokens=False))\n",
        "\n",
        "text_en = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
        "token_ids_en = tokenizer(\n",
        "    text_en,\n",
        "    return_tensors=\"np\",\n",
        "    add_special_tokens=True,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=200\n",
        ")[\"input_ids\"][0]\n",
        "\n",
        "print(\"英語文字列トークン:\")\n",
        "print(token_ids_en)\n",
        "\n",
        "common_elements = np.intersect1d(tokenizer.all_special_ids, token_ids_en)\n",
        "print(\"使用されるスペシャルトークン:\")\n",
        "print(common_elements)\n",
        "\n",
        "\n",
        "text_ja = \"IT業界が属する情報通信業は、総じて若年層の伸びが大きい。 ６０～６４歳層は退職や再雇用による人手不足を補う戦力と位置付けられ、従来より高い賃金が提示されている。製造業がマイナス圏に陥るのは60年代後半まで。ITエンジニアの仕事は多岐にわたり、スキルの有無や習熟度によって賃金は変わります。 ITSSレベルが上がると年収も上がります。賃金は仕事の「価値」に対して支払われます。ある価値を生み出すためには、何らかのスキルが必要だからです。\"\n",
        "token_ids_ja = tokenizer(\n",
        "    text_ja,\n",
        "    return_tensors=\"np\",\n",
        "    add_special_tokens=True,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=200\n",
        ")[\"input_ids\"][0]\n",
        "\n",
        "print(\"日本語文字列トークン:\")\n",
        "print(token_ids_ja)\n",
        "\n",
        "common_elements = np.intersect1d(tokenizer.all_special_ids, token_ids_ja)\n",
        "print(\"使用されるスペシャルトークン:\")\n",
        "print(common_elements)\n"
      ],
      "metadata": {
        "id": "I9HiZCj6jYj7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}