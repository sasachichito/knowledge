![](/picture/自然言語処理1.png)

- コーパス
  - 新聞やネットの記事などの文章や、話し言葉を書き起こした文章などを大量に収集し、検索や分析ができるようにしたデータベースのこと。  
注釈付きコーパスとは、単語の分割境界や品詞タグなどの情報を付加したコーパスを指す。

- 語彙
  - トークンの集合

### 言語モデル
言語モデルとは、入力として文章（トークン列）を受け取り、文章の出現確率（もっともらしさ、出現しやすさ）を返す関数である。  
良いモデルは、人間がもっともらしいと感じる文章に高い確率、つまり1に近い数値を返却する。  
ニューラル言語モデルとは、ニューラルネットワークにより実現される言語モデルのこと。  

```
そもそも確率(Probability)とは、事象の起こりやすさを0以上1以下の数で表したもの。  
サイコロを振って1が出る確率をxとすると、2,3,4,5,6が出る確率も同じため（同様に確からしい、という）  
x + x + x + x + x + x = 1 が成り立ち、x = 1/6 となることからこれは理解できる。

確率の基本は、起こりうる事象を数え上げることにある。

サイコロを2回振って1回目に1、2回目に2が出る確率は、1/6 * 1/6 と覚えているが、これは分母同士の積で起こりうる事象の全パターンを数え上げ、
分子同士の積で対象の事象パターンを数え上げている。
```

言語モデルからすれば、入力された文章Sの出現確率は、Sを構成する複数のトークンが同時に出現する確率といえるため、各トークンの出現率の積を計算することになる。

各トークンの出現率だが、文章には流れ・文脈・テーマがあることから、あるトークンの出現率は、以前に出現したトークンの影響を受けることを考慮する必要がある。これには「条件付き確率」を使う。

以上から、文章Sが3つのトークンw1、w2, w3で構成される場合、文章Sの出現確率p(S)は、次のようになる。

`p(S) = p(w1,w2,w3) = p(w1) * p(w2|w1) * p(w3|w1,w2)`


### 分類器と言語モデル
分類器とは、入力データが与えられたとき、そのカテゴリーを予測するモデルのこと。  
モデルは、入力データとしてxを受け取り、出力としてカテゴリー数と同じ次元を持つベクトルyを返却する。  
yの中で最大値をとる次元に対応するカテゴリが答えとなる。

分類器の仕組みを言語モデルで利用すれば、ある文脈において、次に出現するトークンを予測することができる。  
つまり語彙に含まれる全てのトークン数がNであれば、カテゴリー数をNとするクラス分類と解釈する。

この使い方を言語モデルと表現することも多い。

モデルのパラメータを決定するための評価関数（損失関数）にはクロスエントロピー誤差を用いることが多い。  
これは出力yに非線形関数のSoftMax関数を適用した後、正解となるカテゴリに対応する次元の値を確認し、良し悪しを評価する。


# ニューラル言語モデル

## Word2Vec
Mikolovらによって2013年に提案された、単語に対して文脈非依存の（一意な）分散表現を学習するモデル。  
単語に文脈非依存の分散表現を与えることを、文脈依存の分散表現と対比して、単語埋め込み（Word Embedding）と呼ぶ。　　

Word2Vecにより得られる分散表現は、単語間の意味的類似性だけでなく、`v(日本) - v(東京)　≒ v(フランス) - v(パリ)`、`v(日本) + v(首都) ≒ v(東京)`のような性質を示す。このような数値の加減算と意味的な加減算が一致する性質を加法構成性と呼ぶ。

Word2Vecでは、CBOW（シーバウ）、skip-gramの2つのモデルが提案されている。

ここではSkip-Gramについて説明する。

### モデルのアーキテクチャ
- 入力：語彙に含まれるトークンを一つ指定するone-hotベクトル（一つの要素が1でそれ以外全てが0）
- 出力：語彙の各トークンの出現率を示す確率分布
- 論理処理フロー
  - ①語彙に含まれるトークン数をN、トークンの分散表現ベクトルの次元数をDとしたとき、N * Dの行列E（全トークンのベクトルを持つリスト）から、入力で指定されたトークンのベクトルを取得
  - ②語彙の各トークンについて、①のベクトルとの内積を算出してスコアリストを作成
  - ③スコアリストにSoftmax関数を適用して、語彙の各トークンの出現率を示す確率分布とする
- パーセプトロン構成
  - ![](/picture/自然言語処理2.png)
  - ①の行列Eから指定トークンのベクトルを取得する部分は、入力層でone-hotベクトルを受け取り、入力層の重み（各赤線に付与された重み）を対象トークンのベクトルにすることで実現している。
  - ![](/picture/自然言語処理3.png)
  - ②の内積計算は、語彙の各トークンのベクトルを、中間層の重み（各青線に付与された重み）で表現することで実現している。

※CBOWはSkip-Gramの因果関係を逆転させたモデルで、入力として周辺のトークンリストを受け取り、出力として中心語の予測を返却する。

### モデルの学習方法
- 入力：トークン一つ
- 正解：入力のトークンの周りに出てくるトークン一つ
- ハイパーパラメータ：周りの距離（トークン数）を定義するウィンドウサイズ

正解のトークンに対して、高いスコアが出るように各層を調整する。

## ELMo
2018年に提案されたモデル。Word2Vecとは異なり、ELMoから得られる単語の分散表現は、文脈に応じて異なる値をとる（文脈化単語埋め込み）。

順方向の多層LSTMと、逆方向の多層LSTMを組み合わせた、双方向LSTMで実現される。

### モデルのアーキテクチャ
- 入力：文章（語彙に含まれるトークンの列）
- 出力：語彙の各トークンの出現率を示す確率分布
- 論理処理フロー

### モデルの学習方法
