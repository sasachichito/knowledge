<img src="/picture/自然言語処理1.png" width="100%" />

- コーパス
  - 新聞やネットの記事などの文章や、話し言葉を書き起こした文章などを大量に収集し、検索や分析ができるようにしたデータベースのこと。  
注釈付きコーパスとは、単語の分割境界や品詞タグなどの情報を付加したコーパスを指す。

- 語彙
  - トークンの集合

- トークナイザ
  - ２段階で文章を処理する。
  - ①文章を分割する（分割された一つ一つはこの段階ではトークンではない）
    - 単語分割
      - 単語に分割する。
      - 日本語であれば、Mecab等で、辞書（ipadic等）に含まれる単語に分割する。
    - 文字分割
      - 1文字ずつに分割する。
    - サブワード分割
      - 単語をさらにサブワードに分割する。
      - WordPieceでは、単語の一部（先頭3文字とか）が語彙に含まれているかチェックして、あればサブワードとして切り出すイメージ。
      - 例えば東京タワーは「東京」が語彙に含まれていれば「東京」「##タワー」のサブワードに分割される。先頭でないサブワードには##が付く。
  - ②分割された一つ一つについて、語彙を検索して最長一致するトークンを採用する
    - 語彙になかった場合は[UNK]のようなトークンにする。
    - ①でサブワード分割している場合、「##タワー」などで検索されるため、語彙もサブワードに対応している（##付きのトークンを含む）必要がある。

### 言語モデル
言語モデルとは、入力として文章（トークン列）を受け取り、文章の出現確率（もっともらしさ、出現しやすさ）を返す関数である。  
良いモデルは、人間がもっともらしいと感じる文章に高い確率、つまり1に近い数値を返却する。  
ニューラル言語モデルとは、ニューラルネットワークにより実現される言語モデルのこと。  

```
そもそも確率(Probability)とは、事象の起こりやすさを0以上1以下の数で表したもの。  
サイコロを振って1が出る確率をxとすると、2,3,4,5,6が出る確率も同じため（同様に確からしい、という）  
x + x + x + x + x + x = 1 が成り立ち、x = 1/6 となることからこれは理解できる。

確率の基本は、起こりうる事象を数え上げることにある。

サイコロを2回振って1回目に1、2回目に2が出る確率は、1/6 * 1/6 と覚えているが、これは分母同士の積で起こりうる事象の全パターンを数え上げ、
分子同士の積で対象の事象パターンを数え上げている。
```

言語モデルからすれば、入力された文章Sの出現確率は、Sを構成する複数のトークンが同時に出現する確率といえるため、各トークンの出現率の積を計算することになる。

各トークンの出現率だが、文章には流れ・文脈・テーマがあることから、あるトークンの出現率は、以前に出現したトークンの影響を受けることを考慮する必要がある。これには「条件付き確率」を使う。

以上から、文章Sが3つのトークンw1、w2, w3で構成される場合、文章Sの出現確率p(S)は、次のようになる。

`p(S) = p(w1,w2,w3) = p(w1) * p(w2|w1) * p(w3|w1,w2)`


### 分類器と言語モデル
分類器とは、入力データが与えられたとき、そのカテゴリーを予測するモデルのこと。  
モデルは、入力データとしてxを受け取り、出力としてカテゴリー数と同じ次元を持つベクトルyを返却する。  
yの中で最大値をとる次元に対応するカテゴリが答えとなる。

分類器の仕組みを言語モデルで利用すれば、ある文脈において、次に出現するトークンを予測することができる。  
つまり語彙に含まれる全てのトークン数がNであれば、カテゴリー数をNとするクラス分類と解釈する。

この使い方を言語モデルと表現することも多い。

モデルのパラメータを決定するための評価関数（損失関数）にはクロスエントロピー誤差を用いることが多い。  
これは出力yに非線形関数のSoftMax関数を適用した後、正解となるカテゴリに対応する次元の値を確認し、良し悪しを評価する。

### 学習
モデルを学習させるには、モデルに何かしらのタスクを行わせて（入力に対して出力させて）、その出力の良し悪しを評価する必要がある。  
その評価ができるだけ高くなるように（そのタスクを上手にできるように）モデル内部のパラメーターが調整されていく。  

従って、そもそも正解がわからない（出力を評価できない）タスクを行うモデルは、学習させられない。  
例えば、単語を入力したら、その分散表現を出力するモデルを作っても、そもそも分散表現の正解がわからないから学習させられない。  
でも、文脈を入力したら、次に出てきそうな単語を予測するモデルを作れば、正解がわかるので学習させられる。  

後者のモデルの、学習によって調整された内部パラメータや内部での計算結果を単語の分散表現として扱うことにすれば、学習済みモデルに対し、その処理の部分だけ実行させることで、単語の分散表現を取得できる。  

いずれにせよ、自然言語処理のモデルを考える上では、以下を分けて考える必要がある。
- 学習のためのモデルのアーキテクチャ
- 学習済みモデルから得られるデータや機能


### 単語の分散表現
学習を終えたモデル内部の重みや、途中の計算結果を、単語の分散表現として採用する。

単語の分散表現は、単体では意味を持たない数値配列だが、相関関係があるため比較すると特徴を示す。  
例えば意味の近い2つの単語の分散表現の内積が、似てない単語のそれより大きくなる特徴を示したりする（特徴はモデルのアーキテクチャによって変わる）。  

そうなる仕組みとしては以下の通り。
- モデルは、①入力トークンを示す値と、②調整可能なパラメータを使って計算し、③正解のトークンを示す値を出力する。
- 学習によって、③正解のトークンを示す値を出すために、②調整可能なパラメータが調整され、①入力トークンを示す値と計算される。
- これは③と①が固定で、③＝②*①のような関係が成り立つように調整された②のパラメータの値が、①から③を導くための値であり、①と③の特徴を含む値である（①と③の値が異なれば②も変わる）と理解できる。

ニューラルネットワークなどでは層状に構造化され計算ルールもシンプルなため、②の値を捉えやすい。

後述するWord2Vecで言うと
- モデルは、①入力トークンの示す数値（one-hotベクトル）を受け取り、②NNの重みで計算し、③正解トークンを示す値（分類スコア）を出す。
- 学習によって、③正解トークンを示す値（分類スコア）を出すために、②NNの重みを調整し、①入力トークンの示す数値（one-hotベクトル）と計算する。
- ②NNの重みが、入力トークンから正解トークンを導くための値であり、それをベクトルとして扱っている。
- NNが二層からなるため、それぞれの重みが入力/出力トークンのベクトルとして扱われる。

BERTで言うと
- モデルは、①入力トークンの示す数値（トークンIDリスト）を受け取り、②Transformer層で計算し、③正解トークンを示す値（分類スコア）を出す。
- 学習によって、③正解トークンを示す値（分類スコア）を出すために、②Transformer層のパラメータを調整し、①入力トークンの示す数値（トークンIDリスト）と計算する。
- ②Transformer層のパラメータが、入力トークンリスト（文脈）から正解トークンを導く関係であり、Transformer層の出力をベクトルとして扱っている。
- BERTの構造上、各入力トークンに対応するTransformerの出力があり（文脈考慮した出力）、それがトークンのベクトルとして扱われる。


# ニューラル言語モデル

## Word2Vec
Mikolovらによって2013年に提案された、単語に対して文脈非依存の（一意な）分散表現を学習するモデル。  
単語に文脈非依存の分散表現を与えることを、文脈依存の分散表現と対比して、単語埋め込み（Word Embedding）と呼ぶ。　　

Word2Vecにより得られる分散表現は、単語間の意味的類似性だけでなく、`v(日本) - v(東京)　≒ v(フランス) - v(パリ)`、`v(日本) + v(首都) ≒ v(東京)`のような性質を示す。このような数値の加減算と意味的な加減算が一致する性質を加法構成性と呼ぶ。

Word2Vecでは、CBOW（シーバウ）、skip-gramの2つのモデルが提案されている。

ここでは主にSkip-Gramについて説明する。

### 分類器としてのモデルアーキテクチャと学習方法
- 入力：語彙に含まれるトークンを一つ指定するone-hotベクトル（一つの要素が1でそれ以外全てが0）
- 出力：語彙の各トークンの出現率を示す確率分布
- 論理処理フロー
  - ①語彙に含まれるトークン数をN、トークンの分散表現ベクトルの次元数をDとしたとき、N*Dの行列E（全トークンのベクトルを持つリスト）から、入力で指定されたトークンのベクトルを取得
  - ②語彙の各トークンについて、①のベクトルとの内積を算出してスコアリストを作成
  - ③スコアリストにSoftmax関数を適用して、語彙の各トークンの出現率を示す確率分布にして出力する
- パーセプトロン構成
  - <img src="/picture/自然言語処理2.png" width="50%" />
  - ①の行列Eから指定トークンのベクトルを取得する部分は、入力層でone-hotベクトルを受け取り、入力層の重み（各赤線に付与された重み）を対象トークンのベクトルにすることで実現している。
  - <img src="/picture/自然言語処理3.png" width="50%" />
  - ②の内積計算は、語彙の各トークンのベクトルを、中間層の重み（各青線に付与された重み）で表現することで実現している。
- モデルの学習方法
  - 入力：トークン一つ
  - 正解：入力のトークンの周りに出てくるトークン一つ
  - ハイパーパラメータ：周りの距離（トークン数）を定義するウィンドウサイズ
  - 正解のトークンに対して、高いスコアが出るように各層を調整する。

※CBOWはSkip-Gramの因果関係を逆転させたモデルで、入力として周辺のトークンリスト（multi-hotベクトル）を受け取り、出力として中心語の予測を返却する。  

## ELMo
2018年に提案されたモデル。Word2Vecとは異なり、ELMoから得られる単語の分散表現は、文脈に応じて異なる値をとる（文脈化単語埋め込み）。

※文脈によって単語のベクトルを変えたいという目的があるわけではなく、モデルに文脈を考慮した何かしらのタスクをやらせたいという目的があり、そのタスクで学習させた結果、文脈によってベクトルが変わったという理解が正しい。

順方向の多層LSTMと、逆方向の多層LSTMを組み合わせた、双方向LSTMで実現される。

### 分類器としてのモデルアーキテクチャと学習方法
- 入力：文章（トークンの列）
- 出力：語彙の各トークンの出現率を示す確率分布
- 論理処理フロー
  - a.入力文章の先頭トークンから順に以下を行う
    - ①トークンを文字単位に分割して、文字に割り振った値からベクトルを取得
    - ②次のトークンについて、文字単位に分割してベクトルを取得
    - ③一つ前のトークンのベクトルを②に加味したものをこのトークンのベクトルとする（ここが、文脈に応じてベクトルが変わるということ） 
    - ④②に戻る。入力文章の最後のトークンについて処理が終わったら次に進む。
    - ⑤語彙の各トークンについて、入力文章の最後のトークンのベクトルとの関係（内積等）を算出してスコアリストを作成（Softmax関数を適用した確率分布）
  - b.入力文章の末尾トークンから順に①〜⑤を行う。
  - c.a-⑤と、b-⑤を組み合わせた確率分布を出力する。
- パーセプトロン構成
  - Word2VecのSkip-Gramがたくさん並んで隣と連携しているイメージ（以下図では4つ）
  - <img src="/picture/自然言語処理5.png" width="50%" />
- 学習方法
  - 入力：文章（w1,w2,w3,w4,...,wn）
  - 正解：i番目のトークンwi
  - 正解のトークンに対して、高いスコアが出るように各層を調整する。

学習させたモデルを使えば、入力の各トークンに対する、文脈を考慮した単語分散表現を取得することができる。

<img src="/picture/自然言語処理4.png" width="50%" />

## BERT
BERTは2018年にGoogleにより提案されたモデルで、あるトークンを処理する際に、他のトークンの情報を直接参照することでLSTMよりも長期記憶を実現し、かつ、それぞれのトークン処理を並列実行できることから高い計算効率を誇る。

<img src="/picture/自然言語処理6.png" width="50%" />


詳細に見ると、BERTに対して文章（トークンIDの配列）を与えると、内部で各トークンを768次元ベクトル（これを隠れ層の次元数という）に変換してから処理を開始する（Input Embedding）。  

<img src="/picture/自然言語処理10.png" width="60%" />

<img src="/picture/自然言語処理11.png" width="50%" />

1トークンを768次元ベクトルに変換する手順は以下の通り。
1. （語彙数N,768）行列から、入力トークンと一致する語彙の行ベクトルを取得
2. （2,768）行列から、入力トークンの配置が`[SEP]`の前なら1行目、後ろなら2行目のベクトルを取得
3. （入力可能なトークン数,768）行列から、入力トークンの配置順と一致する行ベクトルを取得
4. 1,2,3のベクトルを足し合わせる。

1,2,3のベクトルの値（各行列の値）は最初は適当なもので、学習で調整されるパラメータである。

1によってトークンの違いを、2によってトークンが属する文章位置（前か後ろか）の違いを、3によってトークンの配置順の違いを、学習において特徴として捉えるようにしている。

BERTは、あるトークンの処理において、他のトークンの情報にどの程度注意を払うかを決めるための、Attention（注意機構）と呼ばれる仕組みを持つ。

BERTは、Transformerというモデルで提案されたTransformer EncoderをAttentionに用いたニューラルネットワーク言語モデルである。

### Transformer Encorder
Transformer Encorderは主に以下で構成される。
- `Multi-Head Attention`（`Scaled Dot-Product Attention`で構成される）
- `Layer Normalization`
- `Feed-forward Network`

<img src="/picture/自然言語処理7.png" width="50%" />


#### Scaled Dot-Product Attention
4つのトークンからなる文章（w1,w2,w3,w4）を入力したとき、w3から（あるいは一つ前の層での出力`x3`から）ベクトル`a3`を算出する際の`Scaled Dot-Product Attention`の流れは以下の通り。  

<img src="/picture/自然言語処理8.png" width="50%" />

- キー`k1`〜`k4`
  - w1〜w4に対応づけられた適当なベクトル
- クエリ`q3`
  - キーとの内積でw3との関係性を表すベクトル（`a3,1`がw3とw1との関係性を示す）
- バリュー`v1`〜`v4`
  - w1〜w4の値を調整するベクトル

#### Multi-Head Attention
`Scaled Dot-Product Attention`を複数回（異なるキー・クエリ・バリューの組を用いて）適用して、最後に出力を一つに集約する。

また、Transformer EncorderではResidual Connectionという接続形式を用いており、次の層にベクトル`a`をそのまま渡すのではなく、元の入力`x`を足してから渡す。これにより、深い層を持つモデルでも学習が適切に行われるようになる。

#### Layer Normalization
ベクトルを正規化して流す。

#### Feed-forward Network
GELU関数を適用して流す。

GELU関数は、ReLU関数をなめらかにしたような関数。

### BERTの学習
<img src="/picture/自然言語処理9.png" width="50%" />


# 評価指標
## 要約タスク
### ROUGE（ルージュ）
https://qiita.com/icoxfog417/items/65faecbbe27d3c53d212


