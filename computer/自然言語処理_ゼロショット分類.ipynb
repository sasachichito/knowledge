{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODzaAGF3cH+jsCCtdMXbKj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasachichito/knowledge/blob/master/computer/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86_%E3%82%BC%E3%83%AD%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%E5%88%86%E9%A1%9E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 依存関係"
      ],
      "metadata": {
        "id": "Fhf2x5jc5r-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfLhDu_U5eYN",
        "outputId": "a83919d9-be94-49e5-8732-c636afafe7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[ja] in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[ja]) (4.66.4)\n",
            "Collecting fugashi>=1.0 (from transformers[ja])\n",
            "  Downloading fugashi-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (600 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipadic<2.0,>=1.0.0 (from transformers[ja])\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidic-lite>=1.0.7 (from transformers[ja])\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidic>=1.0.2 (from transformers[ja])\n",
            "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sudachipy>=0.6.6 (from transformers[ja])\n",
            "  Downloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sudachidict-core>=20220729 (from transformers[ja])\n",
            "  Downloading SudachiDict_core-20240409-py3-none-any.whl (72.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rhoknp<1.3.1,>=1.1.0 (from transformers[ja])\n",
            "  Downloading rhoknp-1.3.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[ja]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[ja]) (4.11.0)\n",
            "Collecting wasabi<1.0.0,>=0.6.0 (from unidic>=1.0.2->transformers[ja])\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting plac<2.0.0,>=1.1.3 (from unidic>=1.0.2->transformers[ja])\n",
            "  Downloading plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja]) (2024.2.2)\n",
            "Building wheels for collected packages: ipadic, unidic, unidic-lite\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=d2885ca04cfab28285e77f8e70b47661f457d2739c9a4a2608837ce7966bb66b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7406 sha256=bd0b18b8e7ce738c4ce3a99f0ec162c846a5344d199ba4af28a21399f136e655\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/72/72/1f3d654c345ea69d5d51b531c90daf7ba14cc555eaf2c64ab0\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=78141ee39dd745f073003693723ec2794c0b977228afa4448b262a7710c7e944\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "Successfully built ipadic unidic unidic-lite\n",
            "Installing collected packages: wasabi, unidic-lite, sudachipy, plac, ipadic, sudachidict-core, rhoknp, fugashi, unidic\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "Successfully installed fugashi-1.3.2 ipadic-1.0.0 plac-1.4.3 rhoknp-1.3.0 sudachidict-core-20240409 sudachipy-0.6.8 unidic-1.1.0 unidic-lite-1.0.8 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[ja]\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def webpage_to_text(url, selector):\n",
        "  text = ''\n",
        "  with urlopen(url) as res:\n",
        "      html = res.read().decode('UTF-8', 'ignore')\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "      # article = soup.find('div', class_=\"articleBody\")\n",
        "      # text = article.get_text(strip=True)\n",
        "      article = soup.select(selector)\n",
        "      text = ''\n",
        "      for p in article:\n",
        "        text += p.get_text(strip=True)\n",
        "      return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ゼロショット分類ライブラリ比較"
      ],
      "metadata": {
        "id": "CdW4JLni6R7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "classifier_mDeBERT = pipeline(\"zero-shot-classification\",model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\", device=device)\n",
        "texts = [\"今日、新しいiPhoneが発売されました\"]\n",
        "labels = [\"スマートフォン\", \"エンタメ\", \"スポーツ\"]\n",
        "result_md = classifier_mDeBERT(texts, labels, hypothesis_template=\"このニュースは{}に関する文章です.\")\n",
        "print(result_md)\n",
        "\n",
        "classifier_bart = pipeline(\"zero-shot-classification\",model=\"facebook/bart-large-mnli\", device=device)\n",
        "texts = [\"Darth Vader\"]\n",
        "labels = [\"Star Wars\", \"Star Trek\"]\n",
        "result_b = classifier_bart(texts, labels, hypothesis_template=\"This person is in {}.\")\n",
        "print(result_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-Eew6Or6Ux5",
        "outputId": "8179827f-890a-4af9-a6b1-dd10404bdbfd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'sequence': '今日、新しいiPhoneが発売されました', 'labels': ['スマートフォン', 'エンタメ', 'スポーツ'], 'scores': [0.9712119102478027, 0.023089371621608734, 0.00569870974868536]}]\n",
            "[{'sequence': 'Darth Vader', 'labels': ['Star Wars', 'Star Trek'], 'scores': [0.9938298463821411, 0.006170147098600864]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "パイプラインから実行(日本語)"
      ],
      "metadata": {
        "id": "DtB0zDs_fRKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "classifier_mDeBERT = pipeline(\"zero-shot-classification\",model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\", device=device)\n",
        "\n",
        "sentence = 'HOME/AINOW編集部/Google I/O 2024におけるAI発表まとめ#Gemini#Google I/O#特集記事2024.05.28Google I/O 2024におけるAI発表まとめ最終更新日：2024年5月28日Google I/O 2024基調講演に登場したスンダ―・ピチャイGoogle CEO。画像出典：Googleブログ記事目次はじめにサマリーGeminiファミリーのアップデートとその活用軽量な1.5 Flash、アップデートしたGemini 1.5 Pro、先進的なProject Astraさらに多機能になる生成検索Gemini 1.5 Proをフル活用するGemini for Google Workspaceその他のGemini活用アプリAIによるAndroidの進化クリエイティブAIの進化ImageFXとImagen 3MusicFXと音楽業界とのコラボレーションVideoFXとVeoVeoとSoraの違い4人のアーティストと「不思議の国のアリス」を生成AIで再構築AI開発者のためのAIGemma 2とPaliGemmaデロリアンが授与されるGemini API デベロッパーコンペティション多数のAI開発ツールが発表＆アップデート第6世代AI用ハードウェア「Trillium」責任あるAIの取り組み拡張されるSynthID社会貢献のためのAIまとめはじめにGoogle主催の開発者カンファレンスGoogle I/O 2024が、日本時間2024年5月15日（アメリカ現地時間5月14日）から2日間の日程で開催されました。OpenAIがGPT-4oを発表した直後に開催された同カンファレンスでは、予想通りGoogleのAI開発に関する新情報が大量に発表されました。本記事では、Google I/O 2024で発表されたGoogleのAI技術とサービスに関する新情報をほぼ網羅的にまとめます。これらの情報はUS版Google公式ブログに設けられた「I/O 2024」関連ブログ記事集を参照したものであり、以下の各見出しの末尾には出典記事を明記します。日本語訳のある記事については、日本語訳記事を出典とします。参考記事：Google I/O 2022で発表された最新自然言語処理技術まとめAIによって自社を全方位的にアップデートしたGoogle I/O 2023まとめサマリー本記事の各見出しで解説する内容は、以下の表のようにまとめられます。見出し名キーワードGeminiファミリーのアップデートとその活用軽量なGemini 1.5 Flash、コンテキストウィンドウを200万にアップデートしたGemini 1.5 Pro、リアルタイム動画による対話を実現したProject Astra、多機能な生成検索AI Overviews、Geminiと連携するGemini for Google Workspace、Gemini 1.5 Proを採用したGemini Advanced、Geminiと対話できるGoogleメッセージ、Geminiと音声で話せるGemini LiveAIによるAndroidの進化教育用AI「LearnLM」と連携するかこって検索、Geminiと連携するオーバーレイ機能・TalkBack、Gemini Nanoの搭載、AIによる詐欺電話の検出、AIが画像を選定するAsk Photos、AIによる盗難防止・詐欺アプリ検出クリエイティブAIの進化画像編集が可能となったImageFX、最新画像生成モデルImagen 3、DJモードを追加したMusicFX、Music AI Sandboxによる音楽業界とのコラボレーション、動画生成アプリVideoFX、Soraに匹敵する動画生成モデルVeo、生成AI時代における実験的アート作品「無限な不思議の国」AI開発者のためのAI軽量かつ高性能なオープンソースモデルGemma 2、最新視覚言語オープンソースモデルPaliGemma、デロリアンが授与されるGeminiAPIデベロッパーコンペティション、AIの安全性を評価するLLM Comparator第6世代AI用ハードウェア「Trillium」前世代よりパフォーマンスが4.7倍となったTPU「Trillium」責任あるAIの取り組みテキストと動画に拡張されるSynthID、LearnLMを活用した教育アプリIlluminateとLearn about、機械学習アルゴリズムによる人間の脳の3D再構築、DNAやRNAも予測可能となったAlphaFold 3、医療に特化したMed-GeminiGeminiファミリーのアップデートとその活用Google I/O 2024においてもっとも重要な発表は、Googleが展開するAIビジネスの根幹となる基盤モデルGeminiのアップデート情報です。以下では、新しい2つのGeminiモデルとGeminiを活用したサービスとアプリをまとめます。軽量な1.5 Flash、アップデートしたGemini 1.5 Pro、先進的なProject AstraGeminiファミリーに効率性を重視したGemini 1.5 Flashが加わりました。APIで提供されるこのモデルは、コンテキストウィンドウが100万トークンでありながら、大量の情報処理を伴う高頻度のタスクに最適化されています。遂行できるタスクには、テキスト要約、チャットアプリケーション、画像や動画のキャプション付け、長い文書や表からのデータ抽出などがあり、マルチモーダル推論にも対応しています。Gemini 1.5 Flashの性能イメージ図。画像出典：Googleブログ記事1.5 Flashは1.5 Proから蒸留という技法を用いて開発されたため、モデルサイズが軽量です。蒸留とはモデルサイズの大きなモデルを開発してから、そのモデルの性能をあまり落とさずにより軽量なモデルを開発する技法です。2024年2月に発表されたGemini 1.5 Proもアップデートされました。コード生成や論理的推論とプランニングが強化され、コンテキストウィンドウが100万から200万に増強されました。ただし、200万のコンテキストウィンドウを使えるのは、ウェイティングリストに登録した開発者とGoogle Cloudのクライアントに限られます。アップデートした1.5 Proは、チャットエージェントのペルソナや応答スタイルを作成したり、複数の関数を呼び出してワークフローを自動化したりすることで、モデルの応答制御を改善しました。Gemini解説サイトには、以上のモデルを含めた既存のGeminiファミリーの以下のようなベンチマークが掲載されています。テスト名Gemini 1.0 ProGemini 1.0 UltraGemini 1.5 Pro（2024年2月版）Gemini 1.5 FlashGemini 1.5 Pro（2024年5月版）MMLU（知識全般）71.8%83.7%81.9%78.9%85.9%Natural2Code（Pythonコード生成）69.6%74.9%77.7%77.2%82.6%MATH（数学）32.6%53.2%58.5%54.9%67.7%GPQA（専門知識）27.9%35.7%41.5%39.5%46.2%Big-Bench Hard（多段階推論）75%83.6%84%85.5%89.2%WMT23（翻訳）71.774.475.274.175.3MMMU（大学レベルの画像問題）47.9%59.4%58.5%56.1%62.2%MathVista（大学レベルの画像問題）45.2%53%52.1%54.3%63.9%EgoSchema（動画Q＆A）55.7%61.5%63.2%63.5%72.2%FLEURS（音声認識）6.4%6%6.6%9.8%6.5%また、ベンチマークをグラフ化すると、以下のようになります。Geminiファミリーは、新しいモデルほど性能が向上していますが、モデルの得手不得手は大きく変わっていないと言えそうです。画像出典：記事著者作成画像出典：記事著者作成1.5 Flashと1.5 Proは、Google AI StudioあるいはVertex AIでパブリックプレビュー提供されています。ほぼリアルタイムでユーザと会話できるAIエージェントのProject Astraも発表されました。Geminiをベースに開発された同AIは、動画を連続的にエンコードして、撮影されたシーンとオブジェクトを認識します。つまり、撮影された環境にもとづいてユーザと会話できるのです。同モデルの性能は、以下の動画を見るとわかります。以上の動画では、スマホから撮影した動画にもとづいた会話に加えて、AI搭載メガネを用いた会話も収録されています。かつてGoogleはGoogle Glassを開発・提供していたので、この製品から学んだノウハウが将来のAI搭載型メガネに活かされるかもしれません。Project Astraと比較できるAIには、2024年5月13日にOpenAIが発表したGPT-4oがあります。後者は前者と同様にカメラで撮影されたオブジェクトをリアルタイムに認識して、ユーザと会話できます。デモ動画を見る限りでは、両者の性能は拮抗しているように思われます。Project AstraとGPT-4oの違いは、サービス展開方法にあります。前者が2024年後半にGoogle製品に搭載される予定なのに対して、後者はすでにChatGPT PlusとTeamsのユーザに段階的に提供され、無料のChatGPTユーザにも制限つきながら提供されています。提供状況に関して言えば後者は先行していますが、前者は多数のGoogle製品に搭載されることで多くのユーザを獲得するポテンシャルがあるでしょう出典記事：Gemini が新たな領域へ : より高速なモデル、ロング コンテキスト、AI エージェントさらに多機能になる生成検索Geminiを検索に応用した生成検索は、2023年のGoogle I/Oではじめて発表されました（※注釈1）。検索結果をAIによって生成した文章で表示するこの検索は、AI Overviews（AI概要）と命名されて改めて公開されました。2024年5月時点ではアメリカ国内限定で利用可能ですが、近日中に多くの国で提供される予定です。（※注釈1）Google I/O 2023における生成検索の詳細は、AINOW特集記事『AIによって自社を全方位的にアップデートしたGoogle I/O 2023まとめ』の見出し「生成系AIと融合する検索」を参照のこと。なおAI Overviewsを利用する場合、これまで通り検索連動広告が表示され、スポンサーによる検索結果と通常のそれを区別するラベルも付与されます。さらに以下の表にまとめられるようにAI Overviews詳細機能の公開も予定されています。AI Overviews詳細機能概要公開時期と公開対象国AI Overviewsの回答をより簡単にしたり、より詳しくしたりする回答調整機能英語版Search Labsから近日中に利用可能複数の質問をまとめて、一括して回答する多段階推論機能英語版Search Labsから近日中に利用可能「1週間の食事メニューを計画する」のような特定の課題に関して、プランニングする機能今年後半追加予定検索内容に対して、AIが生成した観点から情報をまとめた回答を表示する機能アメリカで利用可能撮影された動画をAIが認識して、その動画で問題になっていることに回答するビジュアル検索英語版Search Labsから近日中に利用可能。その後、世界各国に提供出典記事：Generative AI in Search: Let Google do the searching for youGemini 1.5 Proをフル活用するGemini for Google WorkspaceGmailやGoogleドキュメントなどをパッケージ化したサービスであるGoogle Workspaceを、Geminiによって強化したGemini for Google Workspaceもアップデートされました。アップデート内容は、以下の表のようにまとめられます。アップデート項目対象ユーザ提供時期Gmail、Google ドキュメント、Google ドライブ、Google スライド、Google スプレッドシートのサイドパネルの Gemini が Gemini 1.5 Proにアップデート。「〇〇小学校からのメールを要約して」などのような大量の情報を処理できる。Workspace Labs と Gemini for Google Workspace Alphaのユーザ現在利用可能。2024年6月からはGemini for Google Workspace アドオンと Google One AI プレミアム プランからも利用可。Gmailモバイルアプリのメール要約。長いメールスレッドを要約できる。Workspace Labsユーザ2024年5月より利用可能。2024年6月からはGemini for Google Workspace アドオンと Google One AI プレミアム プランからも利用可。モバイル版とPC版のGmailから利用できるContextual Smart Reply。メールスレッドの内容をふまえた返信文を生成できる。Workspace Labsユーザ2024年7月から利用可能GmailのGeminiアイコンから「メールの要約」「返信の提案」などのタスクが実行できる。さらに受信トレイから条件に合致したメールを探したりできるGmail Q&Aも実装。モバイル版とPC版のGmailから利用可能Workspace Labsユーザ2024年7月から利用可能PC版のGmailとGoogleドキュメントから利用できる文章作成をサポートする「Help me write」の対応言語に、英語に加えてスペイン語とポルトガル語が追加。 日本語を含めたその他の言語への対応は、順次追加予定。Gemini for Google Workspaceユーザ2024年5月から数週間以内に利用可能PC版Gmailのサイドパネルを使う様子。画像出典：Googleブログ記事モバイル版GmailのGmail Q＆Aを使う様子。画像出典：Googleブログ記事出典記事：Gemini for Google Workspace で生産性を維持する 3 つの新しい方法その他のGemini活用アプリ検索やGoogle Workspaceのほかにも、以下の表にまとめたようなアプリで最新版のGeminiが活用されます。以下の表におけるGemini AdvancedとはGeminiを活用した対話型AIサービスを意味しており、Google One AI Premium PlanはGemini Advancedを含む複数のサービスをパッケージ化したものです。しかしながら、Gemini Advancedでも2TBのストレージを提供するなどしており、両者の差異はあいまいになっています。サービス/アプリ名Gemini活用概要対象ユーザ提供時期Gemini Advanced基盤モデルにGemini 1.5 Proを採用。コンテキストウィンドウが100万トークンに拡張されたことにより、1,500ページのドキュメントの要約も可能。Googleドライブからのファイルアップデート機能も追加。ファイル内容は、学習データとして使われることはない。Gemini Advancedユーザ2024年5月以降利用可能複雑な計画に関する質問に回答できるようになる。例えば、「レイバーデイに家族とマイアミに旅行する予定。息子はアートが好きで、夫は新鮮なシーフードを食べたいと言っている。Gmail からフライトとホテルの情報を取得して、旅行計画を作成してもらえる？」に回答する。2024年5月から数ヶ月以内で利用戒能Google One AI Premium PlanGeminiの（ジム仲間、コーディングパートナーのような）役割を設定できるGemsが利用可能となる。例えば「ランニング コーチとして、毎日のランニング計画を提案して。ポジティブで、明るく、やる気に満ちた感じで」のような役割設定ができる。150 か国以上の日本語を含む35以上の言語におけるGoogle One AI Premium Planユーザ2024年5月以降まもなくGoogleメッセージAndoird対応アプリのGoogleメッセージから、Geminiと対話できるようになる。英語版Googleメッセージユーザ2024年5月以降利用可能Gemini LiveGemini AdvancedにおけるGeminiとの会話が、音声を通じて行える。2024年後半には動画によるライブチャットに対応。つまり、Geminiが会話している環境を認識するようになる。英語版Gemini Advancedユーザ2024年5月から数ヶ月以内で利用戒能Gemini Liveのイメージ画像。画像出典：Googleブログ記事今後はGoogle カレンダー、Google ToDo リスト、Google Keep などのより多くの Google ツールとGeminiが連携するようになります。ただし、こうした連携は英語から対応することになるので、日本語で利用できるようになるにはしばしの時間を要するでしょう。出典記事：Gemini 1.5 ProをGemini Advancedに搭載AIによるAndroidの進化Android OS搭載スマホは、GeminiをはじめとするAIと連携することでさまざまな新機能が実装されます。Google I/O 2024において発表されたそうした新機能をまとめると、以下の表のようになります。なお、「AI新機能名」は赤字のものは正式名称であり、それ以外は便宜上のものです。また、対応機種については一部で不確定な情報を含みます。AI新機能名機能概要対応機種提供時期かこって検索（Circle to Search）Androidスマホのスクリーン上を線で囲むと、囲まれた箇所のテキストやオブジェクトを認識して検索する。2024年5月より教育用AI「LearnLM」と連携して、教育過程における課題の解決をサポートする（LearnLMの日本語対応は不明）。日本語対応。Google Pixel 6以降の機種、Galaxy S21シリーズ以降の機種で利用可能提供済みGeminiオーバーレイ機能GeminiとAndroidの諸機能が連携する。例えば、Geminiで生成した画像をGmailやGoogleメッセージなどにドラッグ＆ドロップしたり、「この動画を見る」をタップしてYouTubeの動画から特定の情報を探したりできる。数億台のAndroid端末2024年5月から数ヶ月以内Gemini Nanoの搭載Geminiファミリーの最軽量モデルGemini NanoがAndroidに搭載される。その結果、テキスト、画像、音声を一括して処理するマルチモーダルが可能となる。Gemini Nano搭載Android端末2024年後半TalkBackとGemini Nanoとの連携目の不自由なユーザや視力の弱いユーザを音声によってサポートするTalkBackがGemini Nanoと連携する。同AIが不足している情報を認識して補う。Gemini Nano搭載Android端末2024年後半詐欺電話の検出Gemini Nanoが典型的な詐欺電話の会話パターンを検出して、ユーザにアラートを送信する。Gemini Nano搭載Android端末2024年後半Ask Photos（フォトに尋ねる）キャンプをした場所のような画像に関連した質問をすると、Geminiが回答してくれる。旅行で撮影した画像からハイライト画像集を生成したりもできる。この機能は、実験的機能として提供開始される。Android版／iOS版のGoogleフォト、ブラウザ版Googleフォト2024年5月から数ヶ月以内盗難防止機能AIがAndroid端末の盗難に際する動作を検知すると、自動的画面をロックする。盗難された場合、認証に過度に失敗するなどの動作から盗難を特定して、画面をロックする。Android 15 second Beta以上を搭載した機種2024年後半詐欺アプリ対策機能AIが詐欺アプリに典型的な動作パターンを検出すると、Googleに報告をあげる。詐欺アプリと特定された場合、ユーザにアラートを発する。Android 15 second Beta以上を搭載した機種2024年後半かこって検索を実行する様子。かこんだ質問は「あるクルマが8秒間で秒速0メートルから秒速24メートルに加速した。この時の加速度を計算せよ」。この質問にはLearnLMが回答する。画像出典：Google公式ブログ記事Geminiオーバーレイ機能を使う様子。ピックルボールの動画から同スポーツのルールに関する説明を生成している。画像出典：Google公式ブログ記事詐欺電話を検出した時に表示する「詐欺かもしれません。銀行は決して安全のためにあなたのお金を動かすように頼みません」というアラートメッセージ。画像出典：Google公式ブログ記事Ask Photosがユーザが訪れた22の国立公園を撮影した画像から選定したハイライト画像集。画像出典：Google公式ブログ記事出典記事：Experience Google AI in even more ways on AndroidAsk Photos: A new way to search your photos with Gemini10 updates coming to the Android ecosystemクリエイティブAIの進化画像や音楽を生成するクリエイティブAIに関する発表も多数ありました。以下にそれらの発表を表現カテゴリーごとにまとめます。ImageFXとImagen 32024年2月に公開された画像生成アプリImageFXは、新たに画像の一部を編集できる機能を実装しました。同アプリには、DeepMindが開発した画像生成モデルImagen 2が活用されています。このアプリはテキスト入力すると画像を生成しますが、2024年5月時点で日本で利用できるものも、日本語入力では期待した出力が得られず、英語での入力が推奨されます。さらに、今後はImagen 2の後継であるImagen 3が使えるようになります。後者は前者に比べて、入力プロンプトを理解する能力が向上しました。Imagen 3を使うには、ウェイティングリストから申し込む必要があります。ただし2024年5月時点で申し込めるのは、アメリカ在住の18歳以上のユーザに限られます。ImageFXでブラシを使って編集範囲を指定する様子。画像出典：Googleブログ記事MusicFXと音楽業界とのコラボレーションImageFXとともに公開されたテキスト入力から楽曲を生成するMusicFXには、新たにDJモードが追加されました。このモードは、楽曲を構成する楽器の構成配分を調整できるというものです。同アプリも日本で利用できますが、2024年5月時点では英語での入力が推奨されます。また、DeepMindが開発した音楽生成ツールMusic AI Sandboxを用いて、ミュージシャンをはじめとした音楽業界関係者とコラボした活動をまとめた動画が公開されました。さらにグラミー賞受賞ミュージシャンであるワイクリフ・ジーン（Wyclef Jean）らとMusic AI Sandboxを使って制作した楽曲群がYouTubeで公開されました。VideoFXとVeo動画生成アプリVideoFXに関する発表もありました。同アプリは、テキスト入力から最大1分を超える動画を生成するというものです。同アプリには生成したシーンを反復したり、音楽を追加できたりするストーリーモードが実装されています。同アプリを利用するにはウェイティングリストに申し込む必要がありますが、2024年5月時点で申し込めるのはアメリカ在住の18歳以上のユーザに限られます。VideoFXには、DeepMindが開発した動画生成モデルVeoが活用されています。同モデルは「タイムラプス」や「風景の空撮」といった映画用語を理解するうえに、動画全体を通して首尾一貫したシーンやオブジェクトを生成できます。YouTubeには、映画監督のドナルド・グローバー（Donald Glover）が参加したVeoを使ったプロジェクトに関する動画が公開されています。VeoとSoraの違いVeoは、OpenAIが2024年2月に発表した動画生成モデルSoraと遜色のない品質の動画を出力しています。出力動画から両者に優劣をつけたり、出力元モデルを特定するのは難しそうですが、それらのアーキテクチャには明確な違いがあります。Soraでは、動画を学習する過程においてVision Transformerが活用されています（※注釈2）。この技法は、動画をパッチと呼ばれる小片に分割したうえで、動画をテキストシーケンスのように処理するというものです。Sora以前の動画生成モデルでは学習する動画の形式が例えば256×256の解像度で4秒というように制限されていましたが、同技法を活用することでそのような制限がなくなりました。（※注釈2）SoraとVision Transformerの関係については、AINOW翻訳記事『オープンソースのSoraがやってきた！独自のSoraモデルを訓練しよう！』を参照のこと。Soraの訓練時に使われるVision Transformerのイメージ図。画像出典：Sota技術解説ページVeoでは、以下のアーキテクチャ図のように学習する動画を分割することなく処理します。このようなVision Transformerを用いないアーキテクチャは、動画生成の主流となっていました。言わば正統派の同モデルは、Soraには実装されていない領域を指定したマスク編集が可能です。Veoのアーキテクチャ図。画像出典：Veo技術解説ページアーキテクチャの観点からSoraとVeoを比較した場合、動画をテキストライクに処理するSoraは、LLMが急速に進化したことからわかるように、進化に関する大きなポテンシャルを秘めています。対してVeoはストーリーモードやマスク編集を実装していることから、現時点ではSoraより多機能です。それゆえ、アーキテクチャから見れば、両者には一長一短があると言えるかもしれません。4人のアーティストと「不思議の国のアリス」を生成AIで再構築ハルコ・ハヤカワを含む4人のアーティストがImagen 2を活用して制作したアート作品「無限な不思議の国（Infinite Wonderland）」も公開されました。この作品の制作は、生成AI時代における新しい表現を探求することを目的としています。「無限な不思議の国」は、不朽の児童小説『不思議の国のアリス』を構成する1,200の文章すべてを入力プロンプトにして、同小説が描写するシーンを画像として視覚化するアート作品です。画像生成にあたっては、作風を4人のアーティストのなかからひとつを選べます。各アーティストの作風をImagen 2に学習させるにあたっては、わずか十数枚の画像から作風を抽出できる技法であるStyleDropを活用しました。「無限な不思議の国」のトップ画面。画像出典：GoogleLAB SESSIONS「無限な不思議の国」の画像生成画面。画像出典：GoogleLAB SESSIONS出典記事：Introducing VideoFX, plus new features for ImageFX and MusicFXNew generative media models and tools, built with and for creatorsHow four artists used AI to endlessly reimagine “Alice’s Adventures in Wonderland”AI開発者のためのAI以上の発表は、一般ユーザあるいは法人顧客を対象としたものでした。こうしたAIユーザだけではなく、AI開発者向けの発表もありました。以下では、そうした発表をまとめます。Gemma 2とPaliGemmaGeminiファミリーにおけるオープンソースモデルの最新版Gemma 2が発表されました。同モデルのソースコード公開は2024年5月から近日中に公開予定ですが、その特徴は以下の通りです。軽量かつ高性能：Gemma 2は270億パラメータでありながら、Lllma 3 70B（700億パラメータ）の性能に匹敵。改善された運用コスト：NVIDIA GPUに最適化され、Vertex AIで効率的に動作するように調整されているので、運用に必要な計算資源が性能に比して小さい。多様なツールに対応：Google Cloud、Axolotl、Hugging Face、NVIDIA TensorRT-LLM、JAX、Kerasからチューニング可能。Gemma 2をオープンソースモデルのGrok-1、Llma 3 70Bと比較した結果は、以下のグラフのようになります。Gemma 2と著名オープンソースモデルとの性能比較。画像出典：Googleブログ記事オープンソースの視覚言語モデルPaliGemmaも発表されました。視覚言語モデルのPaLI-3にインスパイアされた同モデルは、画像や短い動画のキャプション付け、視覚的な質問応答、画像内のテキスト理解、物体検出、物体分割などを実行できます。同モデルのデモは、Hugging Faceにあります。PaliGemmaはGitHub、Hugging Face models、Kaggle、Vertex AI Model Garden、ai.nvidia.comからアクセスでき、JAXとHugging Face Transformersによる簡単な統合で見つけられます。PaliGemmaがHugging Face Spaceで稼働する様子。画像出典：Googleブログ記事出典記事：Introducing PaliGemma, Gemma 2, and an Upgraded Responsible AI Toolkitデロリアンが授与されるGemini API デベロッパーコンペティションGemini APIを活用したアプリの創造性や実用性を競うGeminiAPIデベロッパーコンペティションの開催が発表されました。同コンペティションの概要は、以下の通りです。大賞受賞者には、1981年式デロリアンのカスタム電気自動車を授与大賞のほかにも影響力、実用性、創造性の各観点における優秀アプリを選出して、賞金を授与AndroidやWebのような各プラットフォームごとにも優秀アプリを選出して、賞金を授与賞金総額は100万ドル審査員が選出する各賞のほかに、一般ユーザの投票によって選出されるピープルズ チョイス アワードもあるアプリ応募締め切りは2024年8月12日、一般投票は8月16日、受賞者の発表は2024年10月を予定応募規定などのコンペ詳細については、コンペティション公式ページを参照してください。出典記事：Gemini API デベロッパーコンペティションで明日に向かって開発する多数のAI開発ツールが発表＆アップデートAI開発ツールに関しても、新規のものからアップデートされたものまでさまざまな発表がありました。そうした発表は、以下の通りです。LLM Comparator：AIモデルの品質と安全性の評価を目的として開発された開発ツール。こちらからデモを試用できる。Vertex AIのアップデート：AIモデル開発プラットフォームVertex AIが大幅アップデート。同プラットフォームからGemini 1.5 Flash、PaliGemmaが利用でき、近日中に200万のコンテキストウィンドウのGemini 1.5 Pro、Imagen 3、Gemma 2も利用可能に。LLM Comparatorの画面。画像出典：Googleブログ記事AI開発ツールを含めた開発者向けのそのほかの発表は、Googleブログ記事「Google I/O 2024の総括：すべての開発者にとってAIを身近で役立つものに」（英文記事）を参照してください。出典記事：Introducing PaliGemma, Gemma 2, and an Upgraded Responsible AI ToolkitVertex AI at I/O: Bringing new Gemini and Gemma models to Google Cloud customers第6世代AI用ハードウェア「Trillium」Googleは10年以上にわたりAI開発用ハードウェアTPU（Tensor Processing Unit）を開発してきました。Google I/O 2024ではTPUシリーズ第6世代となるTrilliumが発表されました。同ハードウェアの主な仕様は、以下の通りです。前世代のTPU v5eと比較して、チップあたりのピークコンピューティングパフォーマンスが4.7倍前世代より高帯域幅メモリ（HBM）の容量と帯域幅が2倍アクセレーターとして第3世代のSparseCoreを搭載することで、大規模なエンベディングを効率化前世代からエネルギー効率が67%向上Trilliumは2024年後半からの提供を予定しています。さらなる詳細は、以下の出典記事を参照のこと。出典記事：第6世代のGoogle Cloud TPU「Trillium」の発表責任あるAIの取り組みGoogleのAI技術とサービスの安全性に対する最新の対策や、社会貢献のために開発されたAIに関する発表もありました。以下では、こうした「責任あるAI」の取り組みをまとめます。拡張されるSynthID画像と音声に対して電子透かしを付与する技術であるSynthID（※注釈3）は、テキストと動画にも活用されるようになりました。（※注釈3）画像と音声に対するSynthIDの詳細は、AINOW特集記事『闇市場も誕生したフェイク画像をめぐる現状と対策まとめ』の見出し「DeepMind開発の電子透かしを付与するSynthID」を参照のこと。テキスト用SynthIDは、任意のテキストがGeminiが生成したものかどうか判定するものです。その仕組みとは、生成される単語（正確にはトークン）の生成確率を調整したうえで、生成されたテキストとその生成確率分布を照合するというものです。簡単に言えば、Gemini特有のテキスト生成傾向を抽出するのです。同技術はGoogle I/O 2024以降、Geminiに導入されます。テキスト用SynthIDはテキストの一部を置き換えるような修正に対しても有効ですが、大幅に書き換えられた場合は判定の信頼性が低下することがあります。テキスト用SynthIDがテキストを判定するイメージ図。画像出典：Googleブログ記事動画用SynthIDは、すべてのフレーム画像に対して画像用SynthIDを埋め込むというものです。同技術は、VideoFXに導入済みです。出典記事：Watermarking AI-generated text and video with SynthID社会貢献のためのAIAndroid搭載の検索技術である「かこって検索」で活用される教育用AIのLearnLMは、以下のような2つの教育用アプリにも活用されています。Illuminateは、研究論文を短い音声会話に分解するアプリです。具体的には、論文の内容を数分の2人の合成音声による会話に変換します。こうした会話に対して、ユーザは質問もできます。このアプリはまだ実験段階にありますが、Lab.googleからウェイティングリストに登録できます。Learn aboutは、AIを活用した最新の教育体験を提供するアプリです。ユーザが特定のトピックについて質問すると、画像、動画、ウェブページ、アクティビティといったさまざまなコンテンツを通して学習をサポートします。同アプリの公式ページに掲載されているウェイティングリストからユーザ登録できますが、2024年5月時点ではアメリカ在住の18歳以上のユーザに限定されています。Learn aboutが動作する様子。画像出典：Googleブログ記事LearnLMのほかにも、Googleは最近、社会貢献のためのAIとして以下のような成果を上げています。人間の脳の一部を3D再構築した。この再構築には、Google研究チームが考案した機械学習アルゴリズムSegCLRアルゴリズムが活用されている。タンパク質構造予測モデルAlphaFoldの最新モデルAlphaFold 3（※注釈4）の開発。タンパク質に加え、DNA、RNA、リガンドの構造と相互作用を予測可能。医療用に特化したGeminiファミリーモデルMed-Geminiの開発。臨床医のレポート作成、医療データの分析等の活用が期待される。（※注釈4）AlphaFoldシリーズについては、AINOW特集記事『AlphaFold開発から見るDeepMindの経営理念』も参照のこと。3D再構築された人間の脳における興奮性ニューロン。画像出典：Googleリサーチブログ記事出典記事：How generative AI expands curiosity and understanding with LearnLMBuilding on our commitment to delivering responsible AIまとめ以上のようにGoogle I/O 2024では、GeminiファミリーをはじめとするGoogle開発の生成AIが、同社の主要サービスに展開される動向が確認できました。こうしたGoogleのAI開発とライバルの筆頭であるOpenAIのそれらと比較した場合、「総合商社的なGoogle」と「専門店的なOpenAI」と表現できるかもしれません。OpenAIのGPT-4oやSoraは、Googleの競合製品と同等以上の性能を実現しています。しかしながら、OpenAIはGoogleほどにはAIサービスを多角的に展開していません。それゆえ、両社に関して単純に優劣をつけることはできないでしょう。今後もGoogleとOpenAIのAI開発競争は継続し、そうした競争にはMetaやMicrosoftのような既存プラットフォーマー、さらにはAnthropicのような新興AI企業も加わることでしょう。こうした競争のなかから、画期的なAI技術とサービスが誕生するのではないでしょうか。記事執筆：吉本 幸記（AINOW翻訳記事担当、JDLA Deep Learning for GENERAL 2019 #1、生成AIパスポート、JDLA Generative AI Test 2023 #2取得）編集：おざけんAINOW編集部AI/DX専門メディア「AINOW（エーアイナウ）」です。AI/DXのプロジェクトに関わる多くの方々に向けて有益な情報を発信し、国内のAI活用とDX推進を加速させていきます。TwitterやFacebookでも活動の様子を発信しています！#Gemini#Google I/O#特集記事トップへ人気の記事（週間）ディープフェイクと生成ディープラーニング Part.3｜ディープフェイクの社会影響と検知技術AIを活用したLINEチャットボットアカウント7選！《AI事例25選》産業別にAIの活用事例をまとめました【保存版】オープンデータ・データセット100選  -膨大なデータを活用しよう！AIは今後私たちにどんな影響を与えるのか？辿ってきた歴史や現状は？AIを題材にした映画9選｜編集部おすすめのAI映画を厳選して紹介！無料メールマガジン登録週1回、注目のAIニュースやイベント情報を編集部がピックアップしてお届けしています。お名前メールアドレス職業任意選択してください会社員会社役員公務員自営業主夫/主婦アルバイト学生その他職種任意選択してくださいWebエンジニアモバイルエンジニアインフラエンジニアその他エンジニアデータアナリスト/データサイエンティストUI/UXデザイナーグラフィックデザイナーその他デザイナーディレクターセールスマーケティングライタープランナーその他の職種​こちらの規約にご同意のうえチェックしてください。規約に同意する同意して送信する​'\n",
        "\n",
        "texts = [sentence]\n",
        "labels = [\"AI\", \"未来\", \"テクノロジー\", \"アーキテクチャ\", \"ビジネス\", \"スポーツ\", \"エンタメ\"]\n",
        "result_md = classifier_mDeBERT(texts, labels, hypothesis_template=\"このニュースは{}に関する文章です.\", multi_label=True)\n",
        "print(result_md[0].get(\"labels\"))\n",
        "print(result_md[0].get(\"scores\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q--xKiEfZ2J_",
        "outputId": "bbc67be8-b5bb-47af-c26c-d5fcf146fed5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['テクノロジー', 'AI', '未来', 'アーキテクチャ', 'ビジネス', 'エンタメ', 'スポーツ']\n",
            "[0.999890923500061, 0.9997818470001221, 0.9997126460075378, 0.9996882677078247, 0.9833944439888, 0.0002405198902124539, 6.651310832239687e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "パイプラインから実行（英語）"
      ],
      "metadata": {
        "id": "wq6R1d5NgFjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "classifier_mDeBERT = pipeline(\"zero-shot-classification\",model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\", device=device)\n",
        "\n",
        "sentence = 'Michael Cohen implicated his former bossDonald Trumpin the hush money scheme to pay Stormy Daniels just days before the 2016 election, saying he doled out $130,000 at Trump’s direction and was promised reimbursement.Cohen’s testimony ties together the prosecution’s allegations that Trump broke the law by falsifying business records to reimburse Cohen and conceal the hush money payment that Cohen said he made at Trump’s direction. Trump has pleaded not guilty and denies having an affair with Daniels.Related live-storyMichael Cohen testifies in Trump hush money trialCohen and Trump mostly avoided eye contact while he testified Monday. Cohen looked directly at prosecutor Susan Hoffinger throughout most of his testimony, occasionally scanning the room or looking in the jury’s direction.'\n",
        "# sentence = '''On May 13, 2024, US time, US OpenAI held an online new product launch event and announced the next-generation AI (artificial intelligence) model \"GPT-4o\". It is a flagship model that will succeed the current model \"GPT-4 turbo\". The keyword is \"speed\". It will enable real-time dialogue between AI and voice. With OpenAI's new move, \"speed\" and \"usability\" are likely to emerge as the next competitive axis for AI models. The \"o\" in GPT-4o comes from the prefix \"omni\", which means \"all\". It can accept any combination of text, voice, images, etc. as input and output. What OpenAI particularly emphasized at the launch event was the improvement of voice dialogue performance. Although voice mode was available in \"ChatGPT\", which used conventional models such as GPT-4, there were issues with waiting time (latency). The average latency was 2.8 seconds for GPT-3.5 and 5.4 seconds for GPT-4. GPT-4o reduced this to an average of 0.32 seconds. The breakthrough was in the model's learning method. Models prior to GPT-4o supported voice mode by combining three independent models. The first model transcribed the voice as text, the second model took in the text and output the answer as text, and the third model converted the text to voice. Because it was converted to text once, it was not possible to incorporate information such as tone of voice or conversations by multiple speakers, and emotions could not be expressed as output. In contrast, GPT-4o is a model trained by combining visual information such as images and videos, text, and voice. By changing the learning method, it achieved low latency and acquired new capabilities to take emotions into account.'''\n",
        "\n",
        "texts = [sentence]\n",
        "# labels = [\"AI\", \"未来\", \"テクノロジー\", \"アーキテクチャ\", \"ビジネス\", \"スポーツ\", \"エンタメ\"]\n",
        "labels = [\"未来\", \"政治\"]\n",
        "result_md = classifier_mDeBERT(texts, labels, hypothesis_template=\"このニュースは{}に関する文章です.\", multi_label=True)\n",
        "print(result_md[0].get(\"labels\"))\n",
        "print(result_md[0].get(\"scores\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV82NxmqgIMH",
        "outputId": "c12db087-7b3b-4158-8fe9-6c8f4481ac6f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['政治', '未来']\n",
            "[0.9821041822433472, 0.0034678068477660418]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "マニュアル操作"
      ],
      "metadata": {
        "id": "cPGiE64kfNTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
        "\n",
        "sentence = 'Who are you voting for in 2020?'\n",
        "labels = ['business', 'art & culture', 'politics']\n",
        "\n",
        "premise = sentence\n",
        "hypothesis = f'This example is {labels[2]}.' # ラベル'politics'の確率を調べる\n",
        "\n",
        "# run through model pre-trained on MNLI\n",
        "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
        "                     truncation_strategy='only_first')\n",
        "print(tokenizer.decode(x.flatten()))\n",
        "print(x)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  x = x.cuda()\n",
        "  nli_model = nli_model.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits = nli_model(x)[0]\n",
        "print(logits)\n",
        "\n",
        "# ロジットのうち、含意（entailment）のロジット（インデックス2）と矛盾（contradiction）のロジット（インデックス0）を抽出します。中立（neutral）のロジット（インデックス1）は無視します。\n",
        "entail_contradiction_logits = logits[:,[0,2]]\n",
        "print(entail_contradiction_logits)\n",
        "\n",
        "# 出したロジットに対してソフトマックス関数を適用し、確率を計算します。\n",
        "probs = entail_contradiction_logits.softmax(dim=1)\n",
        "print(probs)\n",
        "\n",
        "# ソフトマックス後の含意（entailment）の確率を取り出します。この確率が、ラベルが正しい確率として解釈されます。\n",
        "prob_label_is_true = probs[:,1]\n",
        "print(prob_label_is_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noT0ucDRQfw1",
        "outputId": "1240c8d7-5cea-45a9-87ae-54654b3b31b6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2734: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>Who are you voting for in 2020?</s></s>This example is politics.</s>\n",
            "tensor([[    0, 12375,    32,    47,  3434,    13,    11,  2760,   116,     2,\n",
            "             2,   713,  1246,    16,  2302,     4,     2]])\n",
            "tensor([[-2.2515,  1.2416,  1.2983]], device='cuda:0')\n",
            "tensor([[-2.2515,  1.2983]], device='cuda:0')\n",
            "tensor([[0.0279, 0.9721]], device='cuda:0')\n",
            "tensor([0.9721], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}